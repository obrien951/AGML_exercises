{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1. The motivations for reducting a datasets dimensionality include that it takes less data to fit the model, it takes less time to fit the data, and there are fewer parameters in the model which increases variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 2. The curse of dimensionality is the factorial increase in the number of points it takes to cover a space with a higher number of dimensions at regular intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3. It is not, strictly speaking, possible ot project from a lower order space onto a higher space, so the exact values of the features cannot be recovered by backwards projecting onto the higher-dimensional space. However, it is possible to porject values onto that space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 4. PCA will have problems reducing the dimensionality of a highly dimensional dataset. As a math major, I have to say that, strictly speaking, there is no way to truly reduce the dimensionality of a dataset. However, the less a dataset resembles something linear, the harder time that PCA will have coming up with a meaningful basis for a hyperplane that comes close to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5. Im not really sure. It depends on the specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.rand(1000000, 1000) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "rnd_pca = PCA(n_components = 0.95)\n",
    "inc_pca = IncrementalPCA(n_components = 0.95)\n",
    "rbf_pca = KernelPCA(n_components = 0.95)\n",
    "\n",
    "reduced_data = pca.fit_transform(data)\n",
    "print(\"reduced_data.shape \" + str(reduced_data.shape))\n",
    "del reduced_data\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data = np.random.rand(1000, 1000000) * 1000\n",
    "other_reduced_data = pca.fit_transform(other_data)\n",
    "print(other_reduced_data.shape)\n",
    "del other_reduced_data\n",
    "#np.delete(other_reduced_data, np.arange(other_reduced_data.shape[0]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these two instances, it takes ~95 vectors to recover all the variance, but this is random data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incremental PCA. \n",
    "rnd_reduced = rnd_pca.fit_transform(other_data)\n",
    "print(rnd_reduced.shape[0])\n",
    "del rnd_reduced\n",
    "#np.delete(rnd_reduced, np.arange(rnd_reduced.shape[0]), 0)\n",
    "\n",
    "#Randomized PCA\n",
    "inc_reduced = inc_pca.fit_transform(other_data)\n",
    "print(inc_reduced.shape[0])\n",
    "del inc_reduced\n",
    "#np.delete(inc_reduced, np.arange(inc_reduced.shape[0]), 0)\n",
    "\n",
    "#Kernel PCA\n",
    "rbf_reduced = rbf_pca.fit_transform(other_data)\n",
    "print(rbf_reduced.shape[0])\n",
    "del rbf_reduced\n",
    "#np.delete(rbf_reduced)\n",
    "\n",
    "del other_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 7. \n",
    "\n",
    "There are several areas in which these algorithms perform. There is the space savings to be considered, the fidelity to the original data, and the amount of variance explained by the principal components.\n",
    "\n",
    "The explained varience ratio tells the amount of variance in a dataset that lies along a given principal component. \n",
    "\n",
    "One can calculate the ratio between the memory used to store the original data and the compressed data as a measure of the success of compression on the data.\n",
    "\n",
    "One can also calculate a root mean squared error on the distance between the compressed data and the original data or the rms distance between the uncompressed data on the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 8.\n",
    "\n",
    "It doesn't make a lot of sense. The first dimensionality reduction algorithm should get you down to the minimum dimensionality needed to produce a given amount of variance in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rough_time_0 = time.time()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "rough_time_1 = time.time()\n",
    "\n",
    "print(\"time taken to train: \"+str(rough_time_1 - rough_time_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "print(\"accuracy on the full-sized data is: \"+str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to see how the data performs after some PCA reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X, new_y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca95 = PCA(n_components=0.95)\n",
    "red_rnd_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = pca95.fit_transform(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small_train, X_small_test, y_s_train, y_s_test = X_small[:60000], X_small[60000:], new_y[:60000], new_y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_quick_0 = time.time()  \n",
    "red_rnd_clf.fit(X_small_train, y_s_train)\n",
    "time_taken_quick_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time taken to train: \"+str(time_taken_quick_1 - time_taken_quick_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s_pred = red_rnd_clf.predict(X_small_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy on the reduced data is: \"+str(accuracy_score(y_s_test, y_s_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# defaults are sufficient for the exercise, but set them just in case a dev changes their mind\n",
    "tsne = TSNE(n_components=2)\n",
    "X_full = tsne.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
