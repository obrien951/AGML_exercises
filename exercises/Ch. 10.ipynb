{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3.\n",
    "\n",
    "a) They don't output a probability. They only output a classification based on a threshold which gets met based on learned weights that can't be interpreted, so the perceptron can only give us yes or no, and they don't tell us anything intuitive about the system under inspection.\n",
    "\n",
    "b) You can set the activation function to a logistic function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 4.\n",
    "\n",
    "Logistic activations were attractive because they are continuous and differentiable over the entire real line. They "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5.\n",
    "\n",
    "There's the ReLU activation function, the step (heaviside function), the logistic function, and the tanh function. \n",
    "\n",
    "Yes. I can draw them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 6.\n",
    "\n",
    "\n",
    "1.) With n instances, the shape of the input matrix is n $\\times$ 10.    \n",
    "2.) W is $10 \\times 50$ in shape. b is $1 \\times 50$    \n",
    "3.) W is $50 \\times 3$ b is $1 \\times 3$    \n",
    "4.) The output matrix has shape $n \\times 3$    \n",
    "5.) $Y = ReLU(ReLU(XW_0+ B_0)W_h+b_h)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 7.\n",
    "\n",
    "It takes one neuron in the ouptut layer to classify email as ham or spam. You need 10 neurons to classify the mnist data\n",
    "\n",
    "To predict housing prices, there are two ways to do this. One is to \"categorize\" shousing prices into increasingly expensive categories, and this will take $n$ neurons for $n$ levels of expense. For a regression prediction, you only need one neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 8.\n",
    "\n",
    "Backpropagation is a training algorithm. It efficiently computes gradients w.r.t. all weights automatically. Batches of inputs are sent through the Neural network in multiple passes called epochs. The inputs are passed through all the layers of the network, and individual outputs are saved. Output error is measured, and each output connection's contribution to the error is computed using the chain rule. The error contriburtions from the connections in the layer below are then calculated using chain rule, and a gradient descent step is then used to tweak the weights in the network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 9. \n",
    "\n",
    "Hyperparameters in a basic MLP:\n",
    "\n",
    "1.) number of input neurons: can be changed by adding features or by compressing the data    \n",
    "2.) number of hidden layers    \n",
    "3.) neurons per (in each) hidden layer    \n",
    "4.) Hidden layer activation function    \n",
    "5.) Output actiation    \n",
    "6.) Loss function    \n",
    "\n",
    "6.) loss functions include sparse_categorical_crossenropy, mean_squared_error    \n",
    "    __[Keras loss function documentation](https://keras.io/api/losses/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision to Scale\n",
    "\n",
    "The data was unscaled in a preliminary attempt. I'm leaving the choice here because a <tt> uint8 </tt> takes up $\\frac{1}{4}$ the space of a <tt> float</tt>, so the spatial savings might be worth a try without scaling the data.    \n",
    "\n",
    "Since scaling can lead to improvements, and preliminary models only marginally missed the mark, we're going to retry the same tests without scaling. You can retry the tests without scaling by setting to_scale_data to <tt>False</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scale_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "if to_scale_data:\n",
    "    X_train = X_train/255.0\n",
    "    X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_mlp_mnist(n_input_neurons=784, n_hidden_layers=2, l_neurons=[30,30], h_activ=\"relu\", opt=\"adam\", met=\"accuracy\"):\n",
    "    in_l = keras.layers.Flatten(input_shape=[28,28])\n",
    "    hidden_ls = [ keras.layers.Dense(l_neurons[i], activation=h_activ) for i in range(n_hidden_layers) ]\n",
    "    out_l = keras.layers.Dense(10, activation=\"softmax\")\n",
    "    model = keras.models.Sequential([in_l] + hidden_ls + [out_l] )\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer=opt, metrics=[met])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that trains @param mod with a tuple data=(X_train, y_train)\n",
    "# val: validation set\n",
    "def train_mlp(data, mod, eps=25, val = None, cbs=None):\n",
    "    if val:\n",
    "        if cbs:\n",
    "            cb_in = cbs if isinstance(cbs, list) else [cbs]\n",
    "            hist = mod.fit(data[0], data[1], epochs=eps, validation_data=val, callbacks=cb_in)\n",
    "        else:\n",
    "            hist = mod.fit(data[0], data[1], epochs=eps, validation_data=val)        \n",
    "    else:\n",
    "        hist = mod.fit(data[0], data[1], epochs=eps)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3497 - accuracy: 0.8997\n",
      "Epoch 2/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1750 - accuracy: 0.9481\n",
      "Epoch 3/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1377 - accuracy: 0.9598\n",
      "Epoch 4/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1171 - accuracy: 0.9651\n",
      "Epoch 5/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1023 - accuracy: 0.9688\n",
      "Epoch 6/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0923 - accuracy: 0.9722\n",
      "Epoch 7/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0838 - accuracy: 0.9734\n",
      "Epoch 8/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0758 - accuracy: 0.9769\n",
      "Epoch 9/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0695 - accuracy: 0.9784\n",
      "Epoch 10/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0647 - accuracy: 0.9797\n",
      "Epoch 11/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0598 - accuracy: 0.9810\n",
      "Epoch 12/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0557 - accuracy: 0.9819\n",
      "Epoch 13/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0523 - accuracy: 0.9827\n",
      "Epoch 14/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0487 - accuracy: 0.9845\n",
      "Epoch 15/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0459 - accuracy: 0.9848\n",
      "Epoch 16/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0436 - accuracy: 0.9852\n",
      "Epoch 17/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0403 - accuracy: 0.9868\n",
      "Epoch 18/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0371 - accuracy: 0.9876\n",
      "Epoch 19/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0355 - accuracy: 0.9882\n",
      "Epoch 20/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0331 - accuracy: 0.9891\n",
      "Epoch 21/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0327 - accuracy: 0.9892\n",
      "Epoch 22/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0290 - accuracy: 0.9903\n",
      "Epoch 23/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0301 - accuracy: 0.9895\n",
      "Epoch 24/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0272 - accuracy: 0.9908\n",
      "Epoch 25/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0257 - accuracy: 0.9911\n"
     ]
    }
   ],
   "source": [
    "model0 = basic_mlp_mnist()\n",
    "history0 = train_mlp((X_train, y_train), model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 899us/step - loss: 0.1546 - accuracy: 0.9654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15464189648628235, 0.965399980545044]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that on our first attempt, with 2 layers, we got 94.5% accuracy. oof. Let's try adding more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3767 - accuracy: 0.8833\n",
      "Epoch 2/25\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1722 - accuracy: 0.9485\n",
      "Epoch 3/25\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1370 - accuracy: 0.9588\n",
      "Epoch 4/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1170 - accuracy: 0.9642\n",
      "Epoch 5/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1026 - accuracy: 0.9686\n",
      "Epoch 6/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0953 - accuracy: 0.9705\n",
      "Epoch 7/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0855 - accuracy: 0.9733\n",
      "Epoch 8/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0807 - accuracy: 0.9747\n",
      "Epoch 9/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0726 - accuracy: 0.9769\n",
      "Epoch 10/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0698 - accuracy: 0.9780\n",
      "Epoch 11/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0663 - accuracy: 0.9787\n",
      "Epoch 12/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0616 - accuracy: 0.9807\n",
      "Epoch 13/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0563 - accuracy: 0.9823\n",
      "Epoch 14/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0557 - accuracy: 0.9819\n",
      "Epoch 15/25\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0517 - accuracy: 0.9832\n",
      "Epoch 16/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0484 - accuracy: 0.9849\n",
      "Epoch 17/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0476 - accuracy: 0.9847\n",
      "Epoch 18/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0457 - accuracy: 0.9851\n",
      "Epoch 19/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0439 - accuracy: 0.9852\n",
      "Epoch 20/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0411 - accuracy: 0.9861\n",
      "Epoch 21/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0406 - accuracy: 0.9869\n",
      "Epoch 22/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0376 - accuracy: 0.9880\n",
      "Epoch 23/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0372 - accuracy: 0.9882\n",
      "Epoch 24/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0369 - accuracy: 0.9882\n",
      "Epoch 25/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0357 - accuracy: 0.9888\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1543 - accuracy: 0.9636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15432047843933105, 0.9635999798774719]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = basic_mlp_mnist( n_hidden_layers=5, l_neurons=[30,30,30,30,30])\n",
    "history1 = train_mlp((X_train, y_train), model1)\n",
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1543 - accuracy: 0.9636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15432047843933105, 0.9635999798774719]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tantalizingly close. We'll try 10 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0326 - accuracy: 0.9892\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0343 - accuracy: 0.9888\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0321 - accuracy: 0.9903\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0321 - accuracy: 0.9898\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0291 - accuracy: 0.9902\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0294 - accuracy: 0.9902\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0304 - accuracy: 0.9900\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0275 - accuracy: 0.9909\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0285 - accuracy: 0.9908\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0249 - accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "history_1_1 = train_mlp((X_train, y_train), model1, eps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1522 - accuracy: 0.9659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15219201147556305, 0.9659000039100647]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got some imporvement by simply including more layers. We'll try some best practices here and split our training set into training and validation. We'll use the <tt> train_test_split </tt> and then pass it to our training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#bp for best practices\n",
    "#We'll be using default parameters here. We can \n",
    "X_train_bp, X_valid, y_train_bp, y_valid = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.4338 - accuracy: 0.8638 - val_loss: 0.2550 - val_accuracy: 0.9218\n",
      "Epoch 2/25\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.1986 - accuracy: 0.9403 - val_loss: 0.1915 - val_accuracy: 0.9409\n",
      "Epoch 3/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.1557 - accuracy: 0.9527 - val_loss: 0.1879 - val_accuracy: 0.9419\n",
      "Epoch 4/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.1309 - accuracy: 0.9598 - val_loss: 0.1550 - val_accuracy: 0.9526\n",
      "Epoch 5/25\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.1155 - accuracy: 0.9646 - val_loss: 0.1557 - val_accuracy: 0.9521\n",
      "Epoch 6/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0995 - accuracy: 0.9699 - val_loss: 0.1388 - val_accuracy: 0.9589\n",
      "Epoch 7/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0932 - accuracy: 0.9714 - val_loss: 0.1430 - val_accuracy: 0.9602\n",
      "Epoch 8/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.1692 - val_accuracy: 0.9517\n",
      "Epoch 9/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0746 - accuracy: 0.9768 - val_loss: 0.1397 - val_accuracy: 0.9615\n",
      "Epoch 10/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0707 - accuracy: 0.9772 - val_loss: 0.1541 - val_accuracy: 0.9581\n",
      "Epoch 11/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0627 - accuracy: 0.9803 - val_loss: 0.1517 - val_accuracy: 0.9587\n",
      "Epoch 12/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0611 - accuracy: 0.9809 - val_loss: 0.1399 - val_accuracy: 0.9626\n",
      "Epoch 13/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0550 - accuracy: 0.9830 - val_loss: 0.1613 - val_accuracy: 0.9583\n",
      "Epoch 14/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0537 - accuracy: 0.9826 - val_loss: 0.1374 - val_accuracy: 0.9652\n",
      "Epoch 15/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0499 - accuracy: 0.9847 - val_loss: 0.1546 - val_accuracy: 0.9607\n",
      "Epoch 16/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0489 - accuracy: 0.9846 - val_loss: 0.1524 - val_accuracy: 0.9623\n",
      "Epoch 17/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0438 - accuracy: 0.9866 - val_loss: 0.1470 - val_accuracy: 0.9643\n",
      "Epoch 18/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0433 - accuracy: 0.9860 - val_loss: 0.1893 - val_accuracy: 0.9591\n",
      "Epoch 19/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0404 - accuracy: 0.9867 - val_loss: 0.1640 - val_accuracy: 0.9611\n",
      "Epoch 20/25\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.0395 - accuracy: 0.9876 - val_loss: 0.2087 - val_accuracy: 0.9555\n",
      "Epoch 21/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0383 - accuracy: 0.9874 - val_loss: 0.1481 - val_accuracy: 0.9657\n",
      "Epoch 22/25\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.0354 - accuracy: 0.9886 - val_loss: 0.1729 - val_accuracy: 0.9635\n",
      "Epoch 23/25\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.0346 - accuracy: 0.9892 - val_loss: 0.1816 - val_accuracy: 0.9609\n",
      "Epoch 24/25\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.0341 - accuracy: 0.9891 - val_loss: 0.1695 - val_accuracy: 0.9645\n",
      "Epoch 25/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0318 - accuracy: 0.9903 - val_loss: 0.1769 - val_accuracy: 0.9641\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16135148704051971, 0.9674999713897705]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = basic_mlp_mnist( n_hidden_layers=5, l_neurons=[30,30,30,30,30])\n",
    "history2 = train_mlp((X_train_bp, y_train_bp), model2, val=(X_valid, y_valid))\n",
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be more information in the tests than we've gotten so far. We'll try using a deeper neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.5625 - accuracy: 0.8198 - val_loss: 0.2770 - val_accuracy: 0.9175\n",
      "Epoch 2/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.2477 - accuracy: 0.9278 - val_loss: 0.2373 - val_accuracy: 0.9322\n",
      "Epoch 3/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1852 - accuracy: 0.9450 - val_loss: 0.2420 - val_accuracy: 0.9274\n",
      "Epoch 4/25\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.1530 - accuracy: 0.9546 - val_loss: 0.1893 - val_accuracy: 0.9443\n",
      "Epoch 5/25\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.1355 - accuracy: 0.9608 - val_loss: 0.1559 - val_accuracy: 0.9543\n",
      "Epoch 6/25\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.1228 - accuracy: 0.9637 - val_loss: 0.1624 - val_accuracy: 0.9543\n",
      "Epoch 7/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1105 - accuracy: 0.9677 - val_loss: 0.1504 - val_accuracy: 0.9582\n",
      "Epoch 8/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1026 - accuracy: 0.9700 - val_loss: 0.1731 - val_accuracy: 0.9542\n",
      "Epoch 9/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0965 - accuracy: 0.9720 - val_loss: 0.1419 - val_accuracy: 0.9607\n",
      "Epoch 10/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0902 - accuracy: 0.9732 - val_loss: 0.1591 - val_accuracy: 0.9577\n",
      "Epoch 11/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0857 - accuracy: 0.9744 - val_loss: 0.1610 - val_accuracy: 0.9577\n",
      "Epoch 12/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0796 - accuracy: 0.9759 - val_loss: 0.1581 - val_accuracy: 0.9565\n",
      "Epoch 13/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0777 - accuracy: 0.9772 - val_loss: 0.1752 - val_accuracy: 0.9533\n",
      "Epoch 14/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0718 - accuracy: 0.9788 - val_loss: 0.1514 - val_accuracy: 0.9616\n",
      "Epoch 15/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0678 - accuracy: 0.9800 - val_loss: 0.1551 - val_accuracy: 0.9602\n",
      "Epoch 16/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0669 - accuracy: 0.9802 - val_loss: 0.1503 - val_accuracy: 0.9617\n",
      "Epoch 17/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0620 - accuracy: 0.9810 - val_loss: 0.1612 - val_accuracy: 0.9592\n",
      "Epoch 18/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0617 - accuracy: 0.9815 - val_loss: 0.1589 - val_accuracy: 0.9609\n",
      "Epoch 19/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0575 - accuracy: 0.9830 - val_loss: 0.1748 - val_accuracy: 0.9579\n",
      "Epoch 20/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0565 - accuracy: 0.9836 - val_loss: 0.1655 - val_accuracy: 0.9589\n",
      "Epoch 21/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0523 - accuracy: 0.9844 - val_loss: 0.1602 - val_accuracy: 0.9604\n",
      "Epoch 22/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0517 - accuracy: 0.9841 - val_loss: 0.1758 - val_accuracy: 0.9613\n",
      "Epoch 23/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0522 - accuracy: 0.9850 - val_loss: 0.1926 - val_accuracy: 0.9533\n",
      "Epoch 24/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0476 - accuracy: 0.9862 - val_loss: 0.1815 - val_accuracy: 0.9611\n",
      "Epoch 25/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0463 - accuracy: 0.9864 - val_loss: 0.1734 - val_accuracy: 0.9613\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1713 - accuracy: 0.9619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17131701111793518, 0.961899995803833]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = basic_mlp_mnist( n_hidden_layers=9, l_neurons=[30,30,30,30,30,30,30,30,30])\n",
    "history3 = train_mlp((X_train_bp, y_train_bp), model3, val=(X_valid, y_valid))\n",
    "model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training and validation errors seem to be decreasing as the number of epochs passed increases. We'll try to improve this model by training the network for more epochs. We'll make sure to use some kind of early stopping to make sure we don't go too far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save(\"m3_mnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0453 - accuracy: 0.9869 - val_loss: 0.1871 - val_accuracy: 0.9597\n",
      "Epoch 2/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0446 - accuracy: 0.9871 - val_loss: 0.1647 - val_accuracy: 0.9613\n",
      "Epoch 3/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0461 - accuracy: 0.9869 - val_loss: 0.1802 - val_accuracy: 0.9589\n",
      "Epoch 4/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0436 - accuracy: 0.9869 - val_loss: 0.1702 - val_accuracy: 0.9631\n",
      "Epoch 5/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0412 - accuracy: 0.9878 - val_loss: 0.1875 - val_accuracy: 0.9627\n",
      "Epoch 6/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0403 - accuracy: 0.9879 - val_loss: 0.1936 - val_accuracy: 0.9545\n",
      "Epoch 7/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0408 - accuracy: 0.9878 - val_loss: 0.1789 - val_accuracy: 0.9615\n",
      "Epoch 8/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0360 - accuracy: 0.9888 - val_loss: 0.1889 - val_accuracy: 0.9633\n",
      "Epoch 9/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 0.1841 - val_accuracy: 0.9621\n",
      "Epoch 10/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0340 - accuracy: 0.9903 - val_loss: 0.1803 - val_accuracy: 0.9636\n",
      "Epoch 11/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0385 - accuracy: 0.9884 - val_loss: 0.1897 - val_accuracy: 0.9627\n",
      "Epoch 12/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0339 - accuracy: 0.9896 - val_loss: 0.1835 - val_accuracy: 0.9643\n",
      "Epoch 13/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0345 - accuracy: 0.9901 - val_loss: 0.1660 - val_accuracy: 0.9645\n",
      "Epoch 14/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0331 - accuracy: 0.9907 - val_loss: 0.1747 - val_accuracy: 0.9635\n",
      "Epoch 15/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0308 - accuracy: 0.9908 - val_loss: 0.2048 - val_accuracy: 0.9597\n",
      "Epoch 16/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0319 - accuracy: 0.9907 - val_loss: 0.2128 - val_accuracy: 0.9554\n",
      "Epoch 17/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0315 - accuracy: 0.9907 - val_loss: 0.2191 - val_accuracy: 0.9613\n",
      "Epoch 18/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0319 - accuracy: 0.9910 - val_loss: 0.1862 - val_accuracy: 0.9621\n",
      "Epoch 19/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0291 - accuracy: 0.9916 - val_loss: 0.2072 - val_accuracy: 0.9577\n",
      "Epoch 20/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0327 - accuracy: 0.9900 - val_loss: 0.1958 - val_accuracy: 0.9625\n",
      "Epoch 21/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 0.2242 - val_accuracy: 0.9623\n",
      "Epoch 22/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.0298 - accuracy: 0.9916 - val_loss: 0.1868 - val_accuracy: 0.9615\n",
      "Epoch 23/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0289 - accuracy: 0.9919 - val_loss: 0.2097 - val_accuracy: 0.9615\n",
      "Epoch 24/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0269 - accuracy: 0.9924 - val_loss: 0.2177 - val_accuracy: 0.9629\n",
      "Epoch 25/25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0290 - accuracy: 0.9915 - val_loss: 0.2014 - val_accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "model_3_cb = keras.callbacks.ModelCheckpoint(\"m3_mnist.h5\", save_best_only=True)\n",
    "m3_hist = train_mlp((X_train_bp, y_train_bp), model3, val=(X_valid, y_valid), cbs = [model_3_cb] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might not see any improvement just from the initial epochs or from the inclusion of the validation set, so we'll try to use wider layers. This increases the danger of overfitting, but it's worth a shot anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2598 - accuracy: 0.9202\n",
      "Epoch 2/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1170 - accuracy: 0.9651\n",
      "Epoch 3/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0902 - accuracy: 0.9728\n",
      "Epoch 4/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0710 - accuracy: 0.9783\n",
      "Epoch 5/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0601 - accuracy: 0.9817\n",
      "Epoch 6/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0527 - accuracy: 0.9840\n",
      "Epoch 7/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0452 - accuracy: 0.9866\n",
      "Epoch 8/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0405 - accuracy: 0.9877\n",
      "Epoch 9/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0378 - accuracy: 0.9886\n",
      "Epoch 10/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0317 - accuracy: 0.9903\n",
      "Epoch 11/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0308 - accuracy: 0.9904\n",
      "Epoch 12/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0266 - accuracy: 0.9918\n",
      "Epoch 13/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0250 - accuracy: 0.9923\n",
      "Epoch 14/25\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0238 - accuracy: 0.9927\n",
      "Epoch 15/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0228 - accuracy: 0.9931\n",
      "Epoch 16/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0221 - accuracy: 0.9937\n",
      "Epoch 17/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0203 - accuracy: 0.9937\n",
      "Epoch 18/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0204 - accuracy: 0.9941\n",
      "Epoch 19/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0179 - accuracy: 0.9946\n",
      "Epoch 20/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0193 - accuracy: 0.9947\n",
      "Epoch 21/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0178 - accuracy: 0.9951\n",
      "Epoch 22/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0144 - accuracy: 0.9957\n",
      "Epoch 23/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0166 - accuracy: 0.9950\n",
      "Epoch 24/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0145 - accuracy: 0.9955\n",
      "Epoch 25/25\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0148 - accuracy: 0.9954\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1133 - accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11329898983240128, 0.977400004863739]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = basic_mlp_mnist( n_hidden_layers=5, l_neurons=[100,100,100,100,100])\n",
    "history1 = train_mlp((X_train, y_train), model4)\n",
    "model4.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1133 - accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11329898983240128, 0.977400004863739]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model looks promising. We can use a validation set to try to better train a model with this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.2594 - accuracy: 0.9207 - val_loss: 0.1248 - val_accuracy: 0.9644\n",
      "Epoch 2/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.1139 - accuracy: 0.9660 - val_loss: 0.1279 - val_accuracy: 0.9623\n",
      "Epoch 3/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0811 - accuracy: 0.9754 - val_loss: 0.1050 - val_accuracy: 0.9704\n",
      "Epoch 4/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0641 - accuracy: 0.9810 - val_loss: 0.1151 - val_accuracy: 0.9708\n",
      "Epoch 5/25\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.0539 - accuracy: 0.9842 - val_loss: 0.0994 - val_accuracy: 0.9729\n",
      "Epoch 6/25\n",
      "1237/1407 [=========================>....] - ETA: 0s - loss: 0.0446 - accuracy: 0.9861"
     ]
    }
   ],
   "source": [
    "model4_bp = basic_mlp_mnist( n_hidden_layers=5, l_neurons=[300,100,100,100,100])\n",
    "history1 = train_mlp((X_train_bp, y_train_bp), model4_bp,  val=(X_valid, y_valid))\n",
    "model4_bp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_bp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem close. We're going to try to use a smaller learning rate to finish up the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_opt = keras.optimizers.Adam(learning_rate = 0.0005)\n",
    "model4_bp = basic_mlp_mnist( n_hidden_layers=5, l_neurons=[300,300,100,100,100], opt=q_opt)\n",
    "history1 = train_mlp((X_train_bp, y_train_bp), model4_bp,  val=(X_valid, y_valid))\n",
    "model4_bp.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_bp_cb = keras.callbacks.ModelCheckpoint(\"model4_bp.h5\", save_best_only=True)\n",
    "history4_mp = train_mlp((X_train_bp, y_train_bp), model4_bp,  val=(X_valid, y_valid), cbs=model4_bp_cb )\n",
    "model4_bp = keras.models.load_model(\"model4_bp.h5\")\n",
    "model4_bp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_bp_cb = keras.callbacks.ModelCheckpoint(\"model4_bp.h5\", save_best_only=True)\n",
    "history4_mp = train_mlp((X_train_bp, y_train_bp), model4_bp, eps=50, val=(X_valid, y_valid), cbs=model4_bp_cb )\n",
    "model4_bp = keras.models.load_model(\"model4_bp.h5\")\n",
    "model4_bp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_bp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q_opt = keras.optimizers.Adam(learning_rate = 0.00005)\n",
    "\n",
    "\n",
    "model4_bp = basic_mlp_mnist( n_hidden_layers=5, l_neurons=[300,300,100,100,100], opt=q_opt, h_activ=\"tanh\")\n",
    "history1 = train_mlp((X_train_bp, y_train_bp), model4_bp, val=(X_valid, y_valid), eps=50)\n",
    "model4_bp.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = basic_mlp_mnist( n_hidden_layers=3, l_neurons=[100,100,100])\n",
    "history5 = train_mlp((X_train, y_train), model5)\n",
    "model5.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = basic_mlp_mnist( n_hidden_layers=2, l_neurons=[300,300])\n",
    "history6 = train_mlp((X_train, y_train), model6)\n",
    "model6.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = basic_mlp_mnist( n_hidden_layers=2, l_neurons=[300,50])\n",
    "history6 = train_mlp((X_train, y_train), model6)\n",
    "model6.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
